<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>AudioContext & WebAssembly Capabilities Test</title>
</head>
<body>
    <h2>AudioContext & WebAssembly Capabilities Test</h2>

    <label for="renderSize">Enter renderSizeHint (integer or "hardware"):</label>
    <input type="text" id="renderSize" value="hardware"><br><br>

    <label for="latencyHint">Enter latencyHint ("interactive", "playback", "balanced" or a number in seconds for exact):</label>
    <input type="text" id="latencyHint" value="interactive"><br><br>

    <label for="sampleRate">Enter desired sampleRate (leave blank for default):</label>
    <input type="text" id="sampleRate" value=""><br><br>

    <button onclick="testRenderSize()">Test AudioContext</button>
    <button onclick="testWasmCapabilities()">Check WebAssembly Capabilities</button>
    <button onclick="listAudioDevices()">List Audio Devices</button>
    <pre id="output"></pre>

    <script>
        async function listAudioDevices() {
            document.getElementById('output').textContent = ''; // Clear logs
            log("Enumerating audio devices...");
            try {
                // Request permission if not already granted, to get device labels
                await navigator.mediaDevices.getUserMedia({ audio: true });
            } catch (e) {
                log("Microphone access denied or not granted. Device labels may be hidden.");
            }
            const devices = await navigator.mediaDevices.enumerateDevices();
            const audioInputs = devices.filter(d => d.kind === "audioinput");
            const audioOutputs = devices.filter(d => d.kind === "audiooutput");

            log(`Audio Input Devices (${audioInputs.length}):`);
            audioInputs.forEach((device, i) => {
                log(`  [${i}] label: "${device.label || '(no label)'}", id: ${device.deviceId}`);
            });

            log(`Audio Output Devices (${audioOutputs.length}):`);
            audioOutputs.forEach((device, i) => {
                log(`  [${i}] label: "${device.label || '(no label)'}", id: ${device.deviceId}`);
            });
        }

        function log(message) {
            const output = document.getElementById('output');
            output.textContent += message + '\n';
        }

        async function testRenderSize() {
            document.getElementById('output').textContent = ''; // Clear previous logs

            const renderHintInput = document.getElementById('renderSize').value;
            const renderSizeHint = isNaN(renderHintInput) ? renderHintInput : parseInt(renderHintInput, 10);

            const latencyHintInput = document.getElementById('latencyHint').value;
            const latencyHint = (!isNaN(latencyHintInput) && latencyHintInput.trim() !== "")
                ? parseFloat(latencyHintInput)
                : latencyHintInput;

            const sampleRateInput = document.getElementById('sampleRate').value;
            const sampleRate = (!isNaN(sampleRateInput) && sampleRateInput.trim() !== "")
                ? parseInt(sampleRateInput, 10)
                : undefined;

            const contextOptions = {
                renderSizeHint,
                latencyHint
            };
            if (sampleRate !== undefined) contextOptions.sampleRate = sampleRate;

            log(`Requesting microphone access...`);
            let stream;
            let audioCtx;
            try {
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                log(`Microphone access granted.`);

                try {
                    const track = stream.getAudioTracks()[0];
                    const settings = track.getSettings();
                    const devices = await navigator.mediaDevices.enumerateDevices();
                    const selectedDevice = devices.find(
                        d => d.kind === "audioinput" && d.deviceId === settings.deviceId
                    );
                    if (selectedDevice) {
                        log(`Selected audio input device: "${selectedDevice.label || '(no label)'}" (id: ${selectedDevice.deviceId})`);
                    } else {
                        log(`Selected audio input deviceId: ${settings.deviceId} (label unavailable)`);
                    }
                } catch (e) {
                    log("Could not determine selected audio input device.");
                }

                log(`Creating AudioContext with options: ${JSON.stringify(contextOptions)}`);
                try {
                    audioCtx = new AudioContext(contextOptions);
                    log(`AudioContext created successfully.`);
                } catch (error) {
                    log(`Error creating AudioContext: ${error.message}`);
                    stream.getTracks().forEach(track => track.stop());
                    return;
                }

                log(`AudioContext sampleRate: ${audioCtx.sampleRate}`);
                log(`AudioContext baseLatency: ${audioCtx.baseLatency}`);

                const processorCode = `
                    class SizeProcessor extends AudioWorkletProcessor {
                        constructor() {
                            super();
                            this.reported = false;
                        }
                        process(inputs, outputs, parameters) {
                            if (!this.reported) {
                                const bufferSize = outputs[0][0].length;
                                this.port.postMessage({ actualRenderQuantumSize: bufferSize });
                                this.reported = true;
                            }
                            return true;
                        }
                    }
                    registerProcessor('size-processor', SizeProcessor);
                `;

                log(`Adding AudioWorklet module...`);
                try {
                    await audioCtx.audioWorklet.addModule(URL.createObjectURL(new Blob([processorCode], { type: 'application/javascript' })));
                    log(`AudioWorklet module added successfully.`);
                } catch (error) {
                    log(`Error adding AudioWorklet module: ${error.message}`);
                    audioCtx.close();
                    stream.getTracks().forEach(track => track.stop());
                    return;
                }

                log(`Creating AudioWorkletNode...`);
                const workletNode = new AudioWorkletNode(audioCtx, 'size-processor');

                workletNode.port.onmessage = (event) => {
                    log(`Actual render quantum size reported by processor: ${event.data.actualRenderQuantumSize} frames`);
                    audioCtx.close().then(() => log(`AudioContext closed.`));
                    stream.getTracks().forEach(track => track.stop());
                    log(`Microphone stream stopped.`);
                };

                log(`Connecting microphone stream to AudioWorkletNode...`);
                const source = audioCtx.createMediaStreamSource(stream);
                source.connect(workletNode).connect(audioCtx.destination);

            } catch (error) {
                log(`Microphone access denied: ${error.message}`);
                if (stream) stream.getTracks().forEach(track => track.stop());
                if (audioCtx) audioCtx.close();
            }
        }

        async function testWasmCapabilities() {
            document.getElementById('output').textContent = ''; // Clear logs
            log("Checking WebAssembly support...");

            const basicSupport = typeof WebAssembly === "object" &&
                                 typeof WebAssembly.instantiate === "function";
            log("WebAssembly object and instantiate support: " + (basicSupport ? "‚úÖ" : "‚ùå"));

            if (!basicSupport) return;

            const executionSupport = await testWasmExecution();
            log("WebAssembly binary execution test: " + (executionSupport ? "‚úÖ Successful" : "‚ùå Failed"));

            const sharedBufferSupport = typeof SharedArrayBuffer === "function";
            log("SharedArrayBuffer support: " + (sharedBufferSupport ? "‚úÖ" : "‚ùå"));

            const atomicsSupport = typeof Atomics === "object" && typeof Atomics.wait === "function";
            const wasmThreadsSupport = sharedBufferSupport && atomicsSupport;
            log("WebAssembly threads support (SharedArrayBuffer + Atomics): " + (wasmThreadsSupport ? "‚úÖ" : "‚ùå"));

            const wasiSupport = typeof WebAssembly === "object" &&
                WebAssembly.Module &&
                WebAssembly.Module.imports;
            log("WASI support via import inspection: " + (wasiSupport ? "üîç Partial (requires further test)" : "‚ùå"));
        }

        async function testWasmExecution() {
            try {
                const module = new WebAssembly.Module(
                    new Uint8Array([
                        0x00, 0x61, 0x73, 0x6D, // magic number
                        0x01, 0x00, 0x00, 0x00  // version
                    ])
                );
                new WebAssembly.Instance(module);
                return true;
            } catch {
                return false;
            }
        }
    </script>
</body>
</html>

<!-- This HTML file tests the capabilities of AudioContext and WebAssembly in the browser.
     It includes input fields for renderSizeHint and latencyHint, and buttons to test the AudioContext
     and WebAssembly capabilities. The results are displayed in a preformatted text area. -->

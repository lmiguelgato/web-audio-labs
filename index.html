<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Web AudioContext Capability Tester</title>
</head>
<body>
    <h2>AudioContext Capability Tester</h2>

    <label for="renderSize">renderSizeHint (number or "hardware"):</label>
    <input type="text" id="renderSize" value="hardware"><br>

    <label for="latencyHint">latencyHint ("interactive", "playback", "balanced", or number of ms):</label>
    <input type="text" id="latencyHint" value="interactive"><br>

    <label for="sampleRate">sampleRate (Hz):</label>
    <input type="text" id="sampleRate" placeholder="Leave empty for default"><br>

    <button onclick="testAudioContext()">Run Test</button>
    <pre id="output"></pre>

    <script>
        function log(message) {
            document.getElementById('output').textContent += message + '\n';
        }

        async function testAudioContext() {
            const output = document.getElementById('output');
            output.textContent = ''; // clear

            log(`Browser: ${navigator.userAgent}`);
            const renderSizeHintInput = document.getElementById('renderSize').value;
            const latencyHintInput = document.getElementById('latencyHint').value;
            const sampleRateInput = document.getElementById('sampleRate').value;

            const renderSizeHint = isNaN(renderSizeHintInput) ? renderSizeHintInput : parseInt(renderSizeHintInput);
            const sampleRate = sampleRateInput ? parseFloat(sampleRateInput) : undefined;
            const latencyHint = isNaN(latencyHintInput) ? latencyHintInput : parseFloat(latencyHintInput);

            log(`Requesting microphone access...`);
            let stream;
            try {
                stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                log(`Microphone access granted.`);
            } catch (e) {
                log(`Microphone access denied: ${e.message}`);
                return;
            }

            const contextOptions = {
                latencyHint,
                sampleRate,
                renderSizeHint
            };

            log(`Creating AudioContext with options:\n${JSON.stringify(contextOptions, null, 2)}`);
            let audioCtx;
            try {
                audioCtx = new AudioContext(contextOptions);
                log(`AudioContext created.`);
            } catch (e) {
                log(`Error creating AudioContext: ${e.message}`);
                stream.getTracks().forEach(track => track.stop());
                return;
            }

            log(`--- AudioContext Parameters ---`);
            log(`sampleRate (requested: ${sampleRate ?? 'default'}): ${audioCtx.sampleRate}`);
            log(`baseLatency (in seconds): ${audioCtx.baseLatency}`);
            log(`outputLatency (if available): ${audioCtx.outputLatency ?? 'not supported'}`);
            log(`sinkId (if available): ${audioCtx.sinkId ?? 'not supported'}`);

            // Test render quantum size
            const processorCode = `
                class SizeProcessor extends AudioWorkletProcessor {
                    constructor() {
                        super();
                        this.reported = false;
                    }
                    process(inputs, outputs) {
                        if (!this.reported && outputs[0]) {
                            const frames = outputs[0][0]?.length || 0;
                            this.port.postMessage({ actualRenderQuantumSize: frames });
                            this.reported = true;
                        }
                        return true;
                    }
                }
                registerProcessor('size-processor', SizeProcessor);
            `;

            log(`Adding AudioWorklet module...`);
            try {
                await audioCtx.audioWorklet.addModule(URL.createObjectURL(new Blob([processorCode], { type: 'application/javascript' })));
                log(`AudioWorklet module added.`);
            } catch (e) {
                log(`Error adding module: ${e.message}`);
                audioCtx.close();
                stream.getTracks().forEach(track => track.stop());
                return;
            }

            const node = new AudioWorkletNode(audioCtx, 'size-processor');
            node.port.onmessage = (e) => {
                log(`Actual render quantum size: ${e.data.actualRenderQuantumSize} frames`);
                audioCtx.close();
                stream.getTracks().forEach(track => track.stop());
                log(`Test complete. AudioContext closed and stream stopped.`);
            };

            const source = audioCtx.createMediaStreamSource(stream);
            source.connect(node).connect(audioCtx.destination);
        }
    </script>
</body>
</html>
